Extracting Stock Prices of WMT and TSLA

Importing Financial indicators & python libraries

# Installing technical indicator library in python
pip install finta
# Importing libraries
import pandas_datareader.data as web
from finta import TA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
 
# Extracting stock price of both companies
wmt = web.DataReader('WMT','yahoo','1700-01-01')
tsla = web.DataReader('TSLA','yahoo','1700-01-01')
 
# Checking shape of both dataset
print("WMT: ", wmt.shape)
print("TSLA: ",tsla.shape)
Generating new technical indicators for WMT and TSLA
 # Function to create Technical Indicators
 def indicators(df):
  df.rename(columns={"Open":"open","High":"high","Low":"low","Close":"close","Volume":"volume"},inplace=True)
  df = df[["open","high","low","close","Adj Close","volume"]]
 
  df1 = pd.DataFrame()
  # Daily returns
  df1['Daily returns'] = df['Adj Close'].pct_change()
  # Simple moving average
  df1['SMA_5'] = TA.SMA(df, period = 5, column = 'Adj Close')
  df1['SMA_20'] = TA.SMA(df, period = 20, column = 'Adj Close')
  # Exponential moving average
  df1['EMA_5'] = TA.EMA(df, period = 5, column = 'Adj Close')
  df1['EMA_20'] = TA.EMA(df, period = 20, column = 'Adj Close')
  df1['EMA_100'] = TA.EMA(df, period = 100, column = 'Adj Close')
  df1['EMA_200'] = TA.EMA(df, period = 200, column = 'Adj Close')
  # Moving Standard deviation
  df1['MSD'] = TA.MSD(df, period = 20)
  # Bollinger Bands
  df1[['BB_UPPER', 'BB_MIDDLE', 'BB_LOWER']] = TA.BBANDS(df, period = 20, column = 'Adj Close')
  # Kaufman's Adaptive Moving Average (KAMA)
  df1['KAMA_20'] = TA.KAMA(df, period = 20)
  # Moving Average Convergence Divergence (MACD)
  df1[['MACD', 'SIGNAL']] = TA.MACD(df, period_fast = 12, period_slow = 26, signal = 9, adjust = True)
  # Rate-of-Change (ROC)
  df1['ROC'] = TA.ROC(df, period = 1)
  # Relative Strenght Index (RSI)
  df1['RSI'] = TA.RSI(df, period = 14)
  # Average True Range (ATR)
  df1['ATR'] = TA.ATR(df, period = 14)
  # Stochastic Oscillator %K
  df1['%K'] = TA.STOCH(df, period = 14)
  # Stochastic Oscillator %D
  df1['%D'] = TA.STOCHD(df, period = 3, stoch_period = 14)
  # Williams %R
  df1['%R'] = TA.WILLIAMS(df, period = 14)
  # Awesome Oscillator (AO)
  df1['AO'] = TA.AO(df, slow_period = 34, fast_period = 5)
  # Chaikin Oscillator
  df1['CO'] = TA.CHAIKIN(df)
  # Volume Price Trend
  df1['VPT'] = TA.VPT(df)
  # Money Flow Index (MFI)
  df1['MFI'] = TA.MFI(df, period = 14)
 
  return pd.concat([df,df1],axis=1)
 
# New columns has been generated
wmt_t = indicators(wmt)
tsla_t = indicators(tsla)
 
# Viewing columns generated above
print(wmt_t.columns)
print(tsla_t.columns)
 
# Checking shape of the dataframe
previous_wmt = wmt_t.shape
previous_tsla = tsla_t.shape
print("WMT: ",previous_wmt)
print("TSLA: ", previous_tsla)
 
# Filtering data for the analysis of our project
wmt_filter_date = wmt_t["2005-05-31":]
tsla_filter_date = tsla_t["2011-05-24":]
 
# Checking NULL values
print("NULL values in WMT: ",wmt_filter_date.isnull().sum().sum())
print("NULL values in TSLA: ",tsla_filter_date.isnull().sum().sum())
 
# Rounding into four decimals
wmt_final = wmt_filter_date.round(4)
tsla_final = tsla_filter_date.round(4)
# Checking the values rounded to four decimals
print(wmt_final.head(2))
print(tsla_final.head(2))
Creating TARGET variable using ADJ CLOSE
# Creating Binary target variable
def binary_target(df):
  df["shift_close"] = df["Adj Close"].shift(1)
  df["target"] = np.where(df["Adj Close"] >= df["shift_close"],1,0)
 
# Generating TARGET variables using ADJ CLOSE
binary_target(wmt_final)
binary_target(tsla_final)
 
# Removing the first row to eliminate NULL shift close value
wmt_final = wmt_final[1:]
tsla_final = tsla_final[1:]
 
# Dropping ADJ CLOSE
wmt_final.drop(columns=["Adj Close","shift_close"], inplace=True)
tsla_final.drop(columns=["Adj Close","shift_close"], inplace=True)
 
# Checking number of columns
print(wmt_final.shape)
print(tsla_final.shape)
 
# Saving the file into CSV format
wmt_final.to_csv("Walmart_Stock_Data.csv")
tsla_final.to_csv("Tesla_Stock_Data.csv")
 
EXTRACTING HEADLINES FOR WMT AND TSLA
Function to extract WALMART headlines
#Importing libraries
from bs4 import BeautifulSoup
import requests
import pandas as pd
 
# Function to scrap new headlines from given URL
def get_links_wmt(root_url):
    set_of_links = []
    pages = []
 
    # how many pages should we scroll through ? currently set to 196
    for i in range(1, 196):
        r = requests.get(root_url.format(i))
        soup = BeautifulSoup(r.content, 'html5lib')
 
        for li in soup.find_all('a', class_="news-link"):
              set_of_links.append(li.get_text())
              
    return set_of_links
 
# Extracting news headlines and saving into pandas Dataframe and generating CSV file using given URL
hw = pd.DataFrame(get_links_wmt("https://markets.businessinsider.com/news/wmt?p={}"),columns = ["headlines"])
hw.to_csv("headline_wmt.csv")
 
Function to extract TESLA headlines
# Function to scrap new headlines from given URL
def get_links_tsla(root_url):
    set_of_links = []
    pages = []
 
    # how many pages should we scroll through ? currently set to 457
    for i in range(1, 457):
        r = requests.get(root_url.format(i))
        soup = BeautifulSoup(r.content, 'html5lib')
 
        for li in soup.find_all('a', class_="news-link"):
              set_of_links.append(li.get_text())
              
    return set_of_links
 
# Extracting news headlines and saving into pandas Dataframe and generating CSV file using given URL
ht = pd.DataFrame(get_links_tsla("https://markets.businessinsider.com/news/tsla?p={}"),columns = ["headlines"])
ht.to_csv("headline_tsla.csv")
 
Extracting DATES for the above headlines
Dates for WMT
# Function to scrap new dates from given URL
def get_links_wmt_pages(root_url):
    pages = []
 
    # how many pages should we scroll through ? currently set to 196
    for i in range(1, 196):
        r = requests.get(root_url.format(i))
        soup = BeautifulSoup(r.content, 'html5lib')
 
        for pg in soup.find_all('span', class_="warmGrey source-and-publishdate"):
              pages.append(pg.get_text())
 
    return pages
 
# Extracting news dates and saving into pandas Dataframe and generating CSV file using given URL
pw = pd.DataFrame(get_links_wmt_pages("https://markets.businessinsider.com/news/wmt?p={}"),columns = ["pages"])
pw.to_csv("pages_wmt.csv")
 
Dates for TSLA
# Function to scrap new dates from given URL
def get_links_tsla_pages(root_url):
    pages = []
 
    # how many pages should we scroll through ? currently set to 457
    for i in range(1, 457):
        r = requests.get(root_url.format(i))
        soup = BeautifulSoup(r.content, 'html5lib')
 
        for pg in soup.find_all('span', class_="warmGrey source-and-publishdate"):
              pages.append(pg.get_text())
 
    return pages
 
# Extracting news dates and saving into pandas Dataframe and generating CSV file using given URL
pt = pd.DataFrame(get_links_tsla_pages("https://markets.businessinsider.com/news/tsla?p={}"),columns = ["pages"])
pt.to_csv("pages_tsla.csv")

News Headlines & Stock Sentiment
Sentiment Analysis of news headlines for WMT and TSLA

# Importing libraries
import io
from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
# NLTK VADER for sentiment analysis
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
 
# Upload "headline_tsla" & "headline_wmt" file from your local drive
uploaded = files.upload()
 
# Reading uploaded files into python
wmt_headlines = pd.read_csv(io.BytesIO(uploaded['headline_wmt.csv']))
tsla_headlines = pd.read_csv(io.BytesIO(uploaded['headline_tsla.csv']))
 
Function to generate compound analysis and sentiment analysis
# Function to generate graphs using Vader method
def sentiment_analyzer(df, ticker, days):
    df2=df[["Dates","Headlines"]]
 
    vader = SentimentIntensityAnalyzer()
    scores = df2['Headlines'].apply(vader.polarity_scores).tolist()
    scores_df = pd.DataFrame(scores)
    parsed_and_scored_news = df2.join(scores_df, rsuffix='_right')
    parsed_and_scored_news['Dates'] = pd.to_datetime(parsed_and_scored_news.Dates).dt.date
 
    # Last number of days of news headlines analysis with compound
    plt.rcParams['figure.figsize'] = [25, 8]
    mean_scores = parsed_and_scored_news.groupby(['Dates']).mean()
    mean_scores = mean_scores.unstack()
    mean_scores = mean_scores.xs('compound').transpose()
    mean_scores.tail(days).plot(kind = 'bar')
    plt.title(f"'Compound' analysis for {ticker} in the last {days} days")
 
    # Last number of days of new headlines analysis with pos, neu and neg
    TITLE = f"Negative, neutral, and positive sentiment for {ticker} in the last {days} days"
    COLORS = ["red","orange", "green"]
    mean_scores1 = parsed_and_scored_news.groupby(['Dates']).mean()
    plot_day = mean_scores1.drop("compound",axis=1)
 
    return plt.grid(), plot_day.tail(days).plot.bar(stacked = True, figsize=(25, 8), title = TITLE, 
                      color = COLORS).legend(bbox_to_anchor=(0.9, 0.5))
 
# Creating WMT graphs of "compound" and "pos,neg,neu"
sentiment_analyzer(df=wmt_headlines,ticker="WMT",days=90)
 
# Creating TSLA graphs of "compound" and "pos,neg,neu"
sentiment_analyzer(df=tsla_headlines,ticker="TSLA",days=90)
 
Generating Polarity Score to predict the effect of news on the stocks
pip install vaderSentiment
# Importing libraries
import io
import pandas as pd
import numpy as np
from textblob import TextBlob
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from google.colab import files
# Load the data
uploaded = files.upload()
# Store the data into variables
wmt_headlines = pd.read_csv(io.BytesIO(uploaded['headline_wmt.csv']))
# Removing unnecessary columns
new = wmt_headlines.drop(columns="Unnamed: 0")
new.head()
 
# Create a function to get the Subjectivity
def getsubjectivity(text):
  return TextBlob(text).sentiment.subjectivity
 
# Create a function to get the Polarity
def getpolarity(text):
  return TextBlob(text).sentiment.polarity
# Create two new columns "Subjectvity" and "Polarity"
new["Subjectivity"] = new["Headlines"].apply(getsubjectivity)
new["Polarity"] = new["Headlines"].apply(getpolarity)
# Create a function to get the sentiment scores
def getSIA(text):
  sia = SentimentIntensityAnalyzer()
  sentiment = sia.polarity_scores(text)
  return sentiment
 
# Get the sentiment scores for each day
compound = []
neg = []
pos = []
neu = []
SIA = 0
 
for i in range(0, len(new["Headlines"])):
  SIA = getSIA(new["Headlines"][i])
  compound.append(SIA['compound'])
  neg.append(SIA['neg'])
  neu.append(SIA['neu'])
  pos.append(SIA['pos'])
# Store the sentiment score in the new dataset
new["Compound"] = compound
new["Negative"] = neg
new["Neutral"] = neu
new["Positive"] = pos
# Averaging the score by the date and saving into csv file format
csv_files = new.groupby(["Dates"]).mean()
csv_files.to_csv("PolarityScores_TSLA.csv")
# Show the new dataset created with sentiment scores
csv_files.head()
